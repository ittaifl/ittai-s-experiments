# -*- coding: utf-8 -*-
"""ittai's expiriment base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rQRf28RAaxeVu4KQLxZM-jEbr7M_bhSp

# Exersice 13 : Structured Latent Representations Through Predictive Learning
## Neural Systems Fall 2025
## ETH Zürich
### Exercise Teaching Assistants:
- Julien Schmidt (julies1@ethz.ch)
- Raimon Bullich (rbullich@ethz.ch)

# Submission Details
### Student Name : _____ (provide full name here) _____

Submission Date :

Submission Deadline : Monday, 26 May 2025, 11:00 AM

Collaborators : (who did you discuss/collaborate on this exercise with)

### Introduction to the exercise

In this exercise you will learn about structured latent representations and how they can form under a sequence prediction learning regime. The contents of this exercise follow closely the paper Recanatesi et al. (see below) as we will recreate results of figures 1 and 2. As a precursor to starting this assignment, it is recommended to read the paper carefully.

This exercise is split into two sections. Section 1 covers the basics of predictive learning and the emergence of structured latent representations through action and observation in a simple discritised environment. We will implement the predictive card game task from Figure 1 and train a simple feedforward artificial neural network to predict an upcoming sensory observation given a current observation and action. We show that the weights of the hidden layers learn to follow the underlying grid structure of the environment.

In section 2, we will take this further and extend these ideas to a more complicated exvironment where an agent is exploring a 2D environment. By learning to predict next-step sensory observations given current-step action and observations, the agent learnd to build a latent space that is structured and active to the physical environment. The model latent space develops spatially-tuned cells responsive to positions in the environment - akin to place cells in the hippocampus. Further, if next-step prediction is eliminated and favored for simple same-step reconstruction/autoencoding, this phenomena is abolished.

This notebook is intended to be run using google colab and their free GPU runtimes but feel free to run it on a local machine. This notebook is CPU intensive so it is recommended to make use of a GPU. You have access to free GPUs on google colab and you are recommended to use them. Be careful, some GPU runtimes have limited time allowances. Training models and generating datasets can take some time.

### Grading

Follow the grading scheme provided to you in the exercise pdf.There will be 6 points in total.

You only need to add code in the #TO DO sections for the notebook to run and get full points.

Discussion/collaboration with other students regarding this assignment is permitted but you must disclose with whom you worked with. You may collaborate on code but all plots and written answers MUST be your own. To receive credit for this assignment you must submit your own individual notebook with your own individual figures and answers. Shared-submissions between students is not allowed.

### Resources
- Paper:  Recanatesi et al. - Predictive learning as a network mechanism for extracting low-dimensional latent space representations (https://www.nature.com/articles/s41467-021-21696-1)
- Supplementary paper (No need to read): Levinstein et al. - Sequential predictive learning is a unifying theory for hippocampal representation and replay (https://www.biorxiv.org/content/10.1101/2024.04.28.591528v2)
- RatInABox: https://github.com/RatInABox-Lab/RatInABox?tab=readme-ov-file#get-started
"""

!pip install ratinabox

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim import RMSprop
from torch.utils.data import DataLoader, TensorDataset, random_split
import torch.nn.init as init
import matplotlib.pyplot as plt
import matplotlib
from matplotlib.collections import EllipseCollection
from sklearn.decomposition import PCA
from tqdm import tqdm
from ratinabox.Environment import Environment
from ratinabox.Agent import Agent
from ratinabox.Neurons import FieldOfViewBVCs, FieldOfViewOVCs
from ratinabox import utils
import ratinabox
import warnings
warnings.filterwarnings('ignore')
import argparse, os, torch

parser = argparse.ArgumentParser(
    description="Train (or resume) your predictive‐learning experiment"
)
parser.add_argument(
    "--resume",
    type=str,
    default=None,
    help="Path to a .pth checkpoint to resume training from"
)
parser.add_argument(
    "--save-dir",
    type=str,
    default="/content/drive/MyDrive/ittai_experiments/models",
    help="Directory where checkpoints (and final model) are stored"
)
args = parser.parse_args()


"""# Section 1 : Structured latent learning in simple discrete environment"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu" #we force cpu because this dataset is small and colab GPU compute is limited. Feel free to change :)
print(device)

"""### Prepating our dataset

### Training the model
If you emplemented everything correctly, you should see the model training (loss going down) and plots of the latent space.

### Grading:
You will be graded on the plots that are shown during and/or after training. This applies for both conditions of 1) training with action information and 2) training without action information.

Plots to show:
- Training and validation losses across epochs.
- Principle components 1 and 2 of the latent space of the validation set activations. (You should pick 5 epochs to plot for which you show the evolution of the latent space between epochs. It is recommened to plot some early epochs as they show the most change)

These plots should be shown in the notebook and by default will be shown below.

### Model with access to action information

### Model without access to action information

# TO DO
## Question
In max two sentences, comment on the results above and compare the two conditions.

## Answer
...

# Section 2 : Structured latent learning 2D exploration environment

Now we move to replicating results from Figure 2 of Recanatesi et al. The goal of this section is to show the emergence of spatially-tuned cells (neurons of our artificial neural network) from a next-state predictive regime.

We begin with building our 2D environment and agent with Ratinabox similar to the environment used by Recanatesi et al. You are not required to implement anything specific here and simply need to run the environment generation code below. However, it is useful to understand how agent navigates and gathers observations. This is specified in the paper and is replicated here with near exact observation and action spaces. We will go into more detail about these specifics below.

After, we generate our training and testing datasets. These will be used for training, validating, and testing our Recurrent Neural Network. Since our dataset is quite large, generating the datasets will take some time (approx 20-25 minutes). Also there is nothing you need to do here other than run the code. After its time for some training!

We train a Recurrent Neural Network (RNN) where we present to the model as input at each timestep the agents current observation and the next action, and we predict as output the next-step observation. Additionally passed as input is the hidden state from the previous timestep. Here you will have to fill in the missing code for the forward pass. You are given all the variables needed to compute this.

To inspect the actiations of the latent space, we feed in our test dataset and record the activation of the latent layer for each timestep. We can then overlay these activations with the real trajectory of the agent to see if there are any spatially tunned neurons in the latent layer. If neurons do have a spatial preference, we would find place-like activity in neuron's activity tilling the environment. You will have to complete some plots here but are given all the variables you need to plot.

Last, we will compare two following conditions:
- Predictive learning: Where the RNN learns to predict the future observation of the next timestep.
- Autoencoding: Where the RNN simply reconstructs the input of the current timestep.

### Background code for agent and environment
In this notebook, we replicate an environment similar to the 2D environment used in Recanatesi et al. You dont need to go through this code. Its simply background code to run the simulation. Just run it and move to the next step.
##### (MAKE SURE TO RUN)
"""

# THIS IS BACKGROUND CODE FOR INITIALIZING THE AGENT AND ITS VISION CELLS. YOU DO NOT NEED TO UNDERSTAND THIS.
# The class below is responsible for generating field of view cells which are fixed to the wall and activate allong with the
# agent in whichever direction it is facing. If you are curious, the way this class works is by commandeering the Field of View Object Vector Cells
# class in rat in a box and forcing the distance of the cells' receptive fields relative to the agent so that they intersect with the walls of the environment.
# This is a crude but effective way of replicting Recanatesi et al.'s 2D environment and agent.

# Imagine in whichever dierction the agent is facing, it shoots out 6 beams of light at different angles relative to its nose. These beams of light refect off
# the walls and and return to the agent the quantity of red, green and blue at that point of the wall along with the agents distance to the wall at that same
# intersection point. So, for each beam of light, we get 4 values: [R,G,B,distance]. We have 6 beams of beams of light so we get a total of 6*4=24 observations
# for each timestep. These serve as our observations in the environment.

class WallFixedFOVc(FieldOfViewOVCs):
    def __init__(self, Agent, params={}):
        super().__init__(Agent,params)
        self.history["tuning_distances"] = []
        self.history["sigma_distances"] = []
        self.history["sigma_angles"] = []
        self.history["dists"] = []

    def display_vector_cells(self,
                         fig=None,
                         ax=None,
                         t=None,
                         **kwargs):
        """Visualises the current firing rate of these cells relative to the Agent.
        Essentially this plots the "manifold" ontop of the Agent.
        Each cell is plotted as an ellipse where the alpha-value of its facecolor reflects the current firing rate
        (normalised against the approximate maximum firing rate for all cells, but, take this just as a visualisation).
        Each ellipse is an approximation of the receptive field of the cell which is a von Mises distribution in angule and a Gaussian in distance.
        The width of the ellipse in r and theta give 1 sigma of these distributions (for von Mises: kappa ~= 1/sigma^2).

        This assumes the x-axis in the Agent's frame of reference is the heading direction.
        (Or the heading diection is the X-axis for egocentric frame of reference). IN this case the Y-axis is the towards the "left" of the agent.

        Args:
        • fig, ax: the matplotlib fig, ax objects to plot on (if any), otherwise will plot the Environment
        • t (float): time to plot at
        • object_type (int): if self.cell_type=="OVC", which object type to plot

        Returns:
            fig, ax: with the
        """
        if t is None:
            t = self.Agent.history["t"][-1]
        t_id = np.argmin(np.abs(np.array(self.Agent.history["t"]) - t))

        if fig is None and ax is None:
            fig, ax = self.Agent.plot_trajectory(t_start=t - 10, t_end=t, **kwargs)

        pos = self.Agent.history["pos"][t_id]

        y_axis_wrt_agent = np.array([0, 1])
        x_axis_wrt_agent = np.array([1,0])
        head_direction = self.Agent.history["head_direction"][t_id]
        head_direction_angle = 0.0


        if self.reference_frame == "egocentric":
            head_direction = self.Agent.history["head_direction"][t_id]
            # head direction angle (CCW from true North)
            head_direction_angle = (180 / np.pi) * ratinabox.utils.get_angle(head_direction)

            # this assumes the "x" dimension is the agents head direction and "y" is to its left
            x_axis_wrt_agent = head_direction / np.linalg.norm(head_direction)
            y_axis_wrt_agent = utils.rotate(x_axis_wrt_agent, np.pi / 2)


        fr = np.array(self.history["firingrate"][t_id])

        tuning_distances = np.array(self.history["tuning_distances"][t_id])
        sigma_angles = np.array(self.history["sigma_angles"][t_id])
        sigma_distances = np.array(self.history["sigma_distances"][t_id])
        tuning_angles = self.tuning_angles #this is unchanged


        x = tuning_distances * np.cos(tuning_angles)
        y = tuning_distances * np.sin(tuning_angles)

        pos_of_cells = pos + np.outer(x, x_axis_wrt_agent) + np.outer(y, y_axis_wrt_agent)

        ww = sigma_angles * tuning_distances
        hh = sigma_distances
        aa  = 1.0 * head_direction_angle + tuning_angles * 180 / np.pi

        ec = EllipseCollection(ww,hh, aa, units = 'x',
                                offsets = pos_of_cells,
                                offset_transform = ax.transData,
                                linewidth=0.5,
                                edgecolor="dimgrey",
                                zorder = 2.1,
                                )
        if self.cell_colors is None:
            facecolor = self.color if self.color is not None else "C1"
            facecolor = np.array(matplotlib.colors.to_rgba(facecolor))
            facecolor_array = np.tile(np.array(facecolor), (self.n, 1))
        else:
            facecolor_array = self.cell_colors.copy() #made in child class init. Each cell can have a different plot color.
            # e.g. if cells are slective to different object types or however you like
        facecolor_array[:, -1] = 0.7*np.maximum(
            0, np.minimum(1, fr / (0.5 * self.max_fr))
        ) # scale alpha so firing rate shows as how "solid" (up to 0.7 so you can _just_ seen whats beneath) to color of this vector cell is.
        ec.set_facecolors(facecolor_array)
        ax.add_collection(ec)

        return fig, ax


    def ray_distances_to_walls(self, agent_pos, head_direction, thetas, ray_length, walls):
        # This function computes the distance the agent is to the wall relative to its 6 beams of light. We provide the agent position, its
        # head direction and the 6 beams angles. We get as an output the wall distance of each beam relative to the agents position and HD.
        n = len(thetas)

        heading = np.arctan2(head_direction[1], head_direction[0])

        #print(heading)
        # 1) Build the end points of each ray: shape (n,2)
        ends = np.stack([agent_pos + ray_length * np.array([np.cos(heading + θ),np.sin(heading + θ)]) for θ in thetas], axis=0)

        # 3) starts is just the agent_pos repeated: shape (n,2)
        starts = np.tile(agent_pos[np.newaxis, :], (n, 1))

        # 4) build segs as (n_rays, 2, 2), with [p0, p1] = [start, end]
        segs = np.zeros((n, 2, 2))
        segs[:, 0, :] = starts  # p_a0 = start
        segs[:, 1, :] = ends    # p_a1 = end

        # 5) get the raw line parameters
        intercepts = utils.vector_intercepts(segs, walls, return_collisions=False)
        la = intercepts[..., 0]
        lb = intercepts[..., 1]

        # 6) mask for real segment–segment hits
        valid = (la > 0) & (la < 1) & (lb > 0) & (lb < 1)

        # 7) pick the nearest hit along each ray (smallest la)
        la_valid = np.where(valid, la, np.inf)  # invalid → ∞
        min_la = la_valid.min(axis=1)           # one per ray

        # 8) convert parameter to distance
        distances = np.minimum(min_la * ray_length, ray_length)
        #print(distances)
        return distances #as 6x1 vector of distances for each of the 6 sensors

    def update_cell_locations(self):
        # we update the cell locations by forcing them to remain on the walls.
        # A cell should never wander across the open field as they were originally
        # indended to by rat in a box by default.
        thetas = self.Agent.Neurons[0].tuning_angles
        n=len(thetas)

        self.dists = self.ray_distances_to_walls(self.Agent.pos, self.Agent.head_direction, thetas, 100.0, np.array(Env.walls))

        s = self.dists*16.5
        self.tuning_distances = np.ones(n)*0.06*s
        self.sigma_distances = np.ones(n)*0.05
        self.sigma_angles = np.ones(n)*0.83333333/s

    def save_to_history(self):
        super().save_to_history()  # Save standard history
        self.history["tuning_distances"].append(self.tuning_distances.copy())
        self.history["sigma_distances"].append(self.sigma_distances.copy())
        self.history["sigma_angles"].append(self.sigma_angles.copy())
        self.history["dists"].append(self.dists.copy()/np.sqrt(2)) #we divide by sqrt(2) because the model wants numbers between 0 and 1 and the max distance in a 1x1 square is sqrt(2)

    def reset_history(self):
        super().reset_history()
        self.history["dists"]=[]

def get_next_move(coords, hd_idx, m=10):
    #Here we compute the agent's next move. We force the agent to move along a specific grid structure like they do in Recanatesi et al. Each time the agent moves,
    # it randomly selects a next eligable tile out of the 8 tiles adjacent to it. The agent can thus move in any of 8 angles: [N,NE,E,SE,S,SW,W,NE]
    # We dont let it walk into walls.
    wall_buffer = 1
    border_flags = [False,False,False,False] # (E,N,W,S) just a flagging method to check if we are at a border, and how to modulate action choice accordingly
    border_flags[0] = True if coords[0] >= m-wall_buffer else False
    border_flags[1] = True if coords[1] >= m-wall_buffer else False
    border_flags[2] = True if coords[0] <= wall_buffer else False
    border_flags[3] = True if coords[1] <= wall_buffer else False

    #in the following params, default head direction is east. So we adjust default params relative to current HD
    action_probs = np.roll(np.array([0.5, 0.15, 0.05, 0.04, 0.02, 0.04, 0.05, 0.15]),hd_idx) #we adjust probabilities according to current HD
    #action_probs = np.roll(np.array([0.6, 0.15, 0.04, 0.01, 0.0, 0.01, 0.04, 0.15]),hd_idx)
    action_options = np.array(['E','NE','N','NW','W','SW','S','SE'])
    action_idxs = np.array([0,1,2,3,4,5,6,7])
    coord_updates = [(1,0),(1,1),(0,1),(-1,1),(-1,0),(-1,-1),(0,-1),(1,-1)]
    #shift both according to HD

    if np.any(border_flags):
        #Here we remove action options that point to wall. And adjust action probabilities accordingly. Can account for corner options
        if border_flags[0]:#action_probs Wall is East, make all north options impossible
            action_probs[np.where(np.isin(action_options, ['E', 'NE', 'SE']))[0]] = 0
        if border_flags[1]:# Wall is North, make all north options impossible
            action_probs[np.where(np.isin(action_options, ['N', 'NE', 'NW']))[0]] = 0
        if border_flags[2]:# Wall is West, make all north options impossible
            action_probs[np.where(np.isin(action_options, ['W', 'NW', 'SW']))[0]] = 0
        if border_flags[3]:# Wall is South, make all north options impossible
            action_probs[np.where(np.isin(action_options, ['S', 'SE', 'SW']))[0]] = 0

        # Now we recalculate the probabilities of possible actions by normalizing
        action_probs = action_probs/action_probs.sum()

    next_action_idx = np.random.choice(action_idxs, p=action_probs)
    next_coords = coords + coord_updates[next_action_idx]
    action_encoding = np.zeros(8)
    action_encoding[next_action_idx] = 1

    return next_coords, next_action_idx, action_encoding

"""### Initialize agent and environment"""

# Create environment and agent
Env = Environment()
number_of_wall_objects = 12
# objs type 0: RED OBJECTS
t0_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t0_locations = np.where(np.random.rand(len(t0_locations), 1) < 0.5, t0_locations, t0_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=0) for l in t0_locations]
# objs type 1: GREEN OBJECTS
t1_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t1_locations = np.where(np.random.rand(len(t1_locations), 1) < 0.5, t1_locations, t1_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=1) for l in t1_locations]
# objs type 2: PURPLE OBJECTS
t2_locations = np.concatenate((np.round(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1))),(np.random.uniform(low=0.0,high=1.0,size=(number_of_wall_objects,1)))),axis=1)
t2_locations = np.where(np.random.rand(len(t2_locations), 1) < 0.5, t2_locations, t2_locations[:, ::-1])  # randomly swap columns
[Env.add_object(l, type=2) for l in t2_locations]
#Env.plot_environment()

Ag = Agent(Env)

FoV_OVCs_t0 = WallFixedFOVc(Ag, params={"object_tuning_type":0,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})
FoV_OVCs_t1 = WallFixedFOVc(Ag, params={"object_tuning_type":1,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})
FoV_OVCs_t2 = WallFixedFOVc(Ag, params={"object_tuning_type":2,"distance_range": [0.01, .11],
        "spatial_resolution": 0.05,  # resolution of each OVC tiling FoV
        "cell_arrangement": "uniform_manifold",
        "angle_range": [0,125]})

fov_downscalar = 3
FoV_OVCs_t0.tuning_angles = FoV_OVCs_t0.tuning_angles/fov_downscalar
FoV_OVCs_t1.tuning_angles = FoV_OVCs_t1.tuning_angles/fov_downscalar
FoV_OVCs_t2.tuning_angles = FoV_OVCs_t2.tuning_angles/fov_downscalar

m = 64
grid = np.array([(x,y) for x in range(m) for y in range(m)])
grid_centers = np.array([[(x/m ,y/m) for y in range(m) ]for x in range(m)])
coords = np.random.randint(high=m-2,low=1,size=(2))

#unit circle head directions
hds = np.array([(1,0),(np.sqrt(2)/2,np.sqrt(2)/2),(0,1),(-np.sqrt(2)/2,np.sqrt(2)/2),(-1,0),(-np.sqrt(2)/2,-np.sqrt(2)/2),(0,-1),(np.sqrt(2)/2,-np.sqrt(2)/2)])
hds = np.array([(1,0),(np.sqrt(2)/2,np.sqrt(2)/2),(0,1),(-np.sqrt(2)/2,np.sqrt(2)/2),(-1,0),(-np.sqrt(2)/2,-np.sqrt(2)/2),(0,-1),(np.sqrt(2)/2,-np.sqrt(2)/2)])

action_idxs = np.array([0,1,2,3,4,5,6,7])
action_options = np.array(['E','NE','N','NW','W','SW','S','SE'])

action_hd_idx = np.random.randint(high=7,low=0)
Ag.pos = np.array(grid_centers[coords[0],coords[1]],dtype=float)
Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
Ag.dt = 0.05

for i in range(1000):

        coords, action_hd_idx, action_encoding = get_next_move(coords, action_hd_idx,m=m)
        next_pos_update = np.array(grid_centers[coords[0],coords[1]],dtype=float)
        Ag.use_DRIFT = False
        Ag.update(forced_next_position=next_pos_update)
        Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
        Ag.history["head_direction"][-1] = (hds[action_hd_idx].tolist())

        #Red observations
        FoV_OVCs_t0.update_cell_locations()
        FoV_OVCs_t0.update()  # Update neuron firing rates

        #Purple observations
        FoV_OVCs_t1.update_cell_locations()
        FoV_OVCs_t1.update()

        #Green observations
        FoV_OVCs_t2.update_cell_locations()
        FoV_OVCs_t2.update()

print("Please ignore the delay in the agents movements and corresponding observations in this annimation.")
print("The official Ratinabox code is slightly bugged and causes deays. You should at least get an idea of \nhow the agent moves and gathers observations. Plotted here are only red observations but there \nexist also purple and green observations at the same cell locations.")
Ag.animate_trajectory(t_end=3, speed_up=1, additional_plot_func=FoV_OVCs_t0.display_vector_cells)
#Ag.plot_trajectory()

"""### Generate the training dataset"""

#Here we generate a dataset. Rather than training on one long continuous episode, we break it down into many episodes of 100 timesteps.

n_trials = 1000

timesteps = 100 # lenght of each sequence

act_array = np.zeros((n_trials, timesteps, 8))
obs_array = np.zeros((n_trials, timesteps, 24)) # num cells of each FOV group

# loop each episode
for nt in tqdm(np.arange(n_trials)):

    Ag.reset_history() # reset history for every new simulation
    FoV_OVCs_t0.reset_history()
    FoV_OVCs_t1.reset_history()
    FoV_OVCs_t2.reset_history()


    action_hd_idx = np.random.randint(high=7,low=0)
    Ag.pos = np.array(grid_centers[coords[0],coords[1]],dtype=float)
    Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
    Ag.dt = 0.05

    actions_saved = []

    # simulate entire episode
    for t in range(int(timesteps)):
        coords, action_hd_idx, action_encoding = get_next_move(coords, action_hd_idx,m=m)
        next_pos_update = np.array(grid_centers[coords[0],coords[1]],dtype=float)
        Ag.use_DRIFT = False
        Ag.update(forced_next_position=next_pos_update)
        Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
        Ag.history["head_direction"][-1] = (hds[action_hd_idx].tolist())
        actions_saved.append(action_encoding)

        FoV_OVCs_t0.update_cell_locations()
        FoV_OVCs_t0.update()
        FoV_OVCs_t1.update_cell_locations()
        FoV_OVCs_t1.update()
        FoV_OVCs_t2.update_cell_locations()
        FoV_OVCs_t2.update()

    # store trial data
    obs_array[nt] = np.concatenate([FoV_OVCs_t0.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t1.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t2.get_history_arrays()['firingrate'],
                                    FoV_OVCs_t0.get_history_arrays()['dists']], axis=1)
    act_array[nt] = np.array(actions_saved,dtype=float)

"""### Generate testing dataset"""

tmp = 50000 # lenght of simulation

Ag.reset_history() # reset history for every new simulation
FoV_OVCs_t0.reset_history()
FoV_OVCs_t1.reset_history()
FoV_OVCs_t2.reset_history()

action_hd_idx = np.random.randint(high=7,low=0)
Ag.pos = np.array(grid_centers[coords[0],coords[1]],dtype=float)
Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
Ag.dt = 0.05

actions_saved = []

# simulate one trial
for t in tqdm(np.arange(int(tmp))): # time_simulation/Ag.dt gives you all timesteps that the simulation will run.
        # update agent location (through ratinabox)
        coords, action_hd_idx, action_encoding = get_next_move(coords, action_hd_idx,m=m)
        next_pos_update = np.array(grid_centers[coords[0],coords[1]],dtype=float)
        Ag.use_DRIFT = False
        Ag.update(forced_next_position=next_pos_update)
        Ag.head_direction = np.array(hds[action_hd_idx],dtype=float)
        Ag.history["head_direction"][-1] = (hds[action_hd_idx].tolist())
        actions_saved.append(action_encoding)

        FoV_OVCs_t0.update_cell_locations()
        FoV_OVCs_t0.update()
        FoV_OVCs_t1.update_cell_locations()
        FoV_OVCs_t1.update()
        FoV_OVCs_t2.update_cell_locations()
        FoV_OVCs_t2.update()

        # store trial data
obs_array_test = np.concatenate([FoV_OVCs_t0.get_history_arrays()['firingrate'],
                                FoV_OVCs_t1.get_history_arrays()['firingrate'],
                                FoV_OVCs_t2.get_history_arrays()['firingrate'],
                                FoV_OVCs_t0.get_history_arrays()['dists']], axis=1)
act_array_test = actions_saved

# Get positions from ratinabox simulations / check shape of hidden activity
positions = Ag.get_history_arrays()['pos']
Ag.plot_trajectory()

# Check for NaNs
nan_mask = np.isnan(obs_array_test)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array_test= np.nan_to_num(obs_array_test, nan=0.0)

# Convert to torch tensors
obs_seq_test = torch.tensor(obs_array_test, dtype=torch.float32).unsqueeze(1)      # (T, obs_dim)
act_seq_test = torch.tensor(act_array_test, dtype=torch.float32).unsqueeze(1)

"""## Defining the Recurrent Neural Network

Here you will have to complete the forward function of the network.

"""

class NormReLU(nn.Module):
    # Implemented as described in Levinstein et al.
    def __init__(self, hidden_size, epsilon=1e-2, noise_std=0.03):
        super().__init__()
        self.bias = nn.Parameter(torch.zeros(hidden_size))  # learnable bias
        self.epsilon = epsilon
        self.noise_std = noise_std

    def forward(self, x):
        x_norm = (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.epsilon)
        noise = torch.randn_like(x) * self.noise_std
        return F.relu(x_norm + self.bias + noise)

class HardSigmoid(nn.Module):
    # Implemented as described in Recanatesi et al. supplementary material
    def __init__(self):
        super(HardSigmoid, self).__init__()
        self.g = torch.nn.ReLU()

    def forward(self, x):
        return torch.clamp(0.2 * x + 0.5, min=0.0, max=1.0)

class NextStepRNN(nn.Module):
    def __init__(self, obs_dim=144, act_dim=13, hidden_dim=500):
        super().__init__()
        # Save dimensions
        self.obs_dim = obs_dim
        self.act_dim = act_dim
        self.hidden_dim = hidden_dim

        # Layers
        self.W_in = nn.Linear(obs_dim, hidden_dim, bias=False)
        self.W_act = nn.Linear(act_dim, hidden_dim, bias=False)
        self.W_rec = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.W_out = nn.Linear(hidden_dim, obs_dim)  # Output = next observation
        self.beta = nn.Parameter(torch.zeros(1))
        self.norm_relu = NormReLU(hidden_dim)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=0.15)  # 15% input dropout (Table 1)
        self.hardsigmoid = HardSigmoid()

        # We can use different activation functions.
        #self.g = torch.tanh
        self.g = self.norm_relu # From Levenstein et al.
        #self.g = self.hardsigmoid # From Recanatesi et al.

        self.init_weights()

    #Init weights like in Levenstein et al.
    def init_weights(self):
        tau = 2.0
        k_in = 1.0 / np.sqrt(self.obs_dim + self.act_dim)
        k_out = 1.0 / np.sqrt(self.hidden_dim)

        init.uniform_(self.W_in.weight, -k_in, k_in)
        init.uniform_(self.W_act.weight, -k_in, k_in)
        init.uniform_(self.W_out.weight, -k_out, k_out)

        # Identity-initialized + uniform recurrent weights
        W_rec_data = torch.empty(self.hidden_dim, self.hidden_dim)
        init.uniform_(W_rec_data, -k_out, k_out)
        identity_boost = torch.eye(self.hidden_dim) * (1 - 1 / tau)
        W_rec_data += identity_boost
        self.W_rec.weight.data = W_rec_data
    '''
    #Init weights like in Recanatesi et al.
    def init_weights(self):
        # Initialize W_rec to identity matrix
        nn.init.eye_(self.W_rec.weight)

        # Initialize W_in, W_act, and W_out to normal distribution (mean=0, std=0.02)
        nn.init.normal_(self.W_in.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.W_act.weight, mean=0.0, std=0.02)
        nn.init.normal_(self.W_out.weight, mean=0.0, std=0.02)
    '''

    def forward(self, obs_seq, act_seq, return_hidden=False):
        T, B, _ = obs_seq.size()
        h = torch.zeros(B, self.hidden_dim, device=obs_seq.device)
        y = torch.zeros(B, self.obs_dim, device=obs_seq.device)
        outputs, hiddens = [], []

        # Loop through each timestep of the agent's trajectory
        for t in range(T):
            #################### [ TO DO ] ####################
            # [1 point ] Implement the correct computation of the latent layer h
            # Hint: Look to the paper and see how the authors did this.. you can do it in one line of code
            o_in = self.W_in(obs_seq[t,:,:]) # we learn
            a_in = self.W_act(act_seq[t,:,:])
            h_in = self.W_rec(h)
            bias = self.beta
            g = self.g
            h = g(o_in + a_in + h_in + bias)
            # h =
            ###################################################
            if return_hidden:
                hiddens.append(h.detach().cpu())

            y = torch.sigmoid(self.W_out(h))
            outputs.append(y)

        outputs = torch.stack(outputs)  # (T, B, obs_dim)
        if return_hidden:
            hiddens = torch.stack(hiddens)  # (T, B, hidden_dim)
            return outputs, hiddens
        return outputs

def train_next_step_rnn(model, train_loader, val_loader, num_epochs=20, lr=2e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):
    optimizer = RMSprop(model.parameters(), lr=lr, alpha=0.95, eps=1e-8)
    loss_fn = nn.MSELoss()
    loss_ = [] # to store loss per epoch
    patience = 8
    halfing_counter = 0
    epochs_no_improve = 0
    best_val_loss = float('inf')
    l1_lambda = 1e-3
    l1_lambda = 5e-2
    l1_lambda = 0.05
    l1_lambda = 0.0

    for epoch in range(num_epochs):

        epoch_loss = 0.0
        model.train()
        for obs_seq, act_seq, next_obs_seq in train_loader:
            # obs_seq, act_seq, next_obs_seq shape: (T, B, D)
            obs_seq = obs_seq.permute(1, 0, 2).to(device)  # From (B, T, D) --> (T, B, D)
            act_seq = act_seq.permute(1, 0, 2).to(device)
            next_obs_seq = next_obs_seq.permute(1, 0, 2).to(device)

            optimizer.zero_grad()
            pred = model(obs_seq, act_seq)  # shape: (T, B, obs_dim)
            mse_loss = loss_fn(pred, next_obs_seq)

            loss = mse_loss

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        loss_.append(avg_loss)

        # Validation step.
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for obs_seq, act_seq, next_obs_seq in val_loader:
                # obs_seq, act_seq, next_obs_seq shape: (T, B, D)
                obs_seq = obs_seq.permute(1, 0, 2).to(device)  # From (B, T, D) --> (T, B, D)
                act_seq = act_seq.permute(1, 0, 2).to(device)
                next_obs_seq = next_obs_seq.permute(1, 0, 2).to(device)

                pred = model(obs_seq, act_seq)  # shape: (T, B, obs_dim)
                mse_loss = loss_fn(pred, next_obs_seq)

                loss = mse_loss

                val_loss += loss.item()
        val_avg_loss = val_loss / len(val_loader)

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}: Train Loss = {avg_loss:.6f}, Val Loss = {val_avg_loss:.6f}")

         # Learning rate reduction if no improvement.
        if val_loss < best_val_loss - 5e-5:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.5
                print("Reducing learning rate to", optimizer.param_groups[0]['lr'])
                epochs_no_improve = 0
                halfing_counter+=1
        if halfing_counter >= 5:
            break

    return loss_

# RUN WITH FROZEN PARAMETERS
def extract_hidden_activity_frozen_weights(model, obs_seq, act_seq):
    model.eval()
    # Move input tensors to the same device as the model
    obs_seq = obs_seq.to(device)
    act_seq = act_seq.to(device)
    with torch.no_grad():
        pred, hiddens = model(obs_seq, act_seq, return_hidden=True)
    return pred, hiddens.cpu().numpy()  # shape: (T, hidden_dim)


def plot_latent_space_spatial_activity(model, obs_seq_test, act_seq_test, positions_to_use=None):
    if positions_to_use is None:
        positions_to_use = positions  # Use global positions if none provided

    pred, hidden_activity = extract_hidden_activity_frozen_weights(model, obs_seq_test[0:-2,:,:], act_seq_test[1:-1,:,:])
    pred = pred.reshape(pred.shape[0], pred.shape[2])
    hidden_activity = hidden_activity.reshape(hidden_activity.shape[0], hidden_activity.shape[2])

    T = hidden_activity.shape[0]
    nbins = 32
    activity_map = np.zeros((hidden_dim, nbins, nbins))
    counts = np.zeros((nbins, nbins))
    hidden_np = hidden_activity
    for t in range(T):
        x_bin = int(positions_to_use[t, 0] * nbins)  # Use the correct positions
        y_bin = int(positions_to_use[t, 1] * nbins)
        if 0 <= x_bin < nbins and 0 <= y_bin < nbins:
            activity_map[:, y_bin, x_bin] += hidden_np[t]
            counts[y_bin, x_bin] += 1
    # Normalize to get average firing rates
    for i in range(hidden_dim):
        activity_map[i] /= (counts + 1e-5)
    np.random.seed(42)
    units_to_plot = 100
    fig = plt.figure(figsize=(10,10))

    ##################### [ TO DO ] ####################
    # TO DO: plot the activity heatmap of each neuron in the hidden layer of the RNN
    for i in range(units_to_plot):
        ax= fig.add_subplot(10,10, i+1)

        ax.imshow(activity_map[i], origin='lower', cmap='jet',interpolation='gaussian')
        #ax.imshow( ?? , origin='lower', cmap='jet',interpolation='gaussian')

        ax.set(xticks=[], yticks=[])
    fig.tight_layout()
    plt.show()
    ####################################################

# Check for NaNs
nan_mask = np.isnan(obs_array)
has_nan = nan_mask.any().item() # Are there any NaNs?
nan_count = nan_mask.sum().item() # Count of NaNs
nan_indices = np.argwhere(nan_mask)
print("Nan sanity check:",has_nan, nan_count, nan_indices)
obs_array= np.nan_to_num(obs_array, nan=0.0)

# Convert to torch tensors
obs_seq = torch.tensor(obs_array[:,:-1], dtype=torch.float32).to(device)      # (num_trials, T-1, obs_dim)
act_seq = torch.tensor(act_array[:,1:], dtype=torch.float32).to(device)          # (num_trials, T-1, act_dim)
next_obs_seq = torch.tensor(obs_array[:,1:], dtype=torch.float32).to(device)  # (num_trials, T-1, act_dim)

# Training dataset for normal predict regime
full_dataset = TensorDataset(obs_seq, act_seq, next_obs_seq)

val_fraction = 0.2  # 20% validation
val_size = int(len(full_dataset) * val_fraction)
train_size = len(full_dataset) - val_size
train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=128, num_workers=0, pin_memory=False)
val_loader = DataLoader(val_dataset, batch_size=128, num_workers=0, pin_memory=False)


# Training dataset for autoencoding regime
full_dataset_autoencoding = TensorDataset(obs_seq, torch.zeros_like(act_seq), obs_seq)

train_dataset_autoencoding, val_dataset_autoencoding = random_split(full_dataset_autoencoding, [train_size, val_size])
train_loader_autoencoding = DataLoader(train_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)
val_loader_autoencoding = DataLoader(val_dataset_autoencoding, batch_size=128, num_workers=0, pin_memory=False)

# check shape of each batch
for batch in train_loader:
    obs, act, next_obs = batch
    print("obs shape:", obs.shape)
    print("act shape:", act.shape)
    print("next_obs shape:", next_obs.shape)
    break  # Just check the first batch

# TRAIN MODEL
obs_dim = obs_seq.shape[-1]
act_dim = act_seq.shape[-1]
hidden_dim = 100
# Initialize model
model = NextStepRNN(obs_dim=obs_dim, act_dim=act_dim, hidden_dim=hidden_dim)
# ——— Resume from a checkpoint if `--resume` was passed ———
if args.resume and os.path.isfile(args.resume):
    print(f"⏳ Resuming from checkpoint {args.resume}")
    model.load_state_dict(torch.load(args.resume, map_location=device))

loss_ = train_next_step_rnn(model, train_loader, val_loader, num_epochs=800, lr=0.01)
# ——— Save into args.save_dir ———
os.makedirs(args.save_dir, exist_ok=True)
out_path = os.path.join(args.save_dir, "model_final.pth")
torch.save(model.state_dict(), out_path)
print(f"✅ Saved final weights to {out_path}")
#################### [ TO DO ] ####################
# Plot the loss across epochs of training
plt.plot(loss_)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()
###################################################

############### [ TO DO IN FUNCTION] ##############
# Plot the activity of cells relative to
plot_latent_space_spatial_activity(model, obs_seq_test, act_seq_test, positions)
###################################################